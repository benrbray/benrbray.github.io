---
title: Matrix Calculus
summary: 'DESCRIPTION'
datePublished: '29 May 2024'

series:
  seriesId: breadcrumbs-game-physics
  seriesNumber: 2
  isAppendix: true
---

import PostContent from "../../../components/blog/PostContent.astro";
import SeriesSummary from "../../../components/content/SeriesSummary.astro";

$$
\providecommand{\norm}[1]{\left\lVert #1 \right\rVert}
\providecommand{\d}{\mathrm{d}}
\providecommand{\diffd}[2]{\frac{\d #1}{\d #2}}
\providecommand{\diffp}[2]{\frac{\partial #1}{\partial #2}}
$$

Linear algebra is *not* about number crunching on matrices, even though some introductory courses make it seem that way.  In this section, we'll review the basics of linear algebra and matrix calculus, focusing on just a handful general principles that can be used to derive useful facts as needed, avoiding unnecessary memorization.

<PostContent>
  <SeriesSummary />
</PostContent>

# Derivatives

Our goal in this section is to understand what it measn to take the derivative of a matrix-valued function of a matrix-valued input, $f : \R^m \rightarrow \R^n$.  We roughly follow [@rudin1976:principles, Chapter 9].

In this section, we briefly review single-variable calculus before introducing derivatives for functions of multiple variables.

## Real-Valued Functions of a Single Variable

## Real-Valued Functions of a Vector Variable

## Vector-Valued Functions of a Single Variable

## Vector-Valued Functions of a Vector Variable

Our intuition for single-variable functions about derivatives measuring "slope" is suitable for understanding the concept of [partial derivatives](https://en.wikipedia.org/wiki/Partial_derivative), but we need to develop new tools to allow us to take derivatives in _all directions simultaneously_.  Here is the key fact to remember:

> The **derivative** is a linear transformation with the same domain and codomain as the original function.

**Definition:** Let $f : \R^n \rightarrow \R^n$ and suppose $x \in \R^n$.  A **(total) derivative** of the function $f : \R^n \rightarrow \R^m$ at the point $x \in \R^n$ is a linear transformation $A \in L(\R^n, \R^m)$ such that

$$
\lim_{h \rightarrow 0} \frac{\norm{f(x _ h) - f(x) - Ah}}{\norm{h}} = 0
$$

**Property:** The derivative, when it exists, is unique.

When $A$ is the derivative of $f$ at $x$, we may use any of the following equivalent notations:

$$
\begin{gather*}
\d_x f = A &&
f'(x) = A &&
\textstyle\frac{\d f}{\d x}\big\rvert_{x} = A
\end{gather*}
$$

In one dimension, the derivative was defined to be exactly equal to a certain limit, giving a nice formula for computing derivatives directly.  Unfortunately, our definition for multivariate functions isn't so nice for direct computation, since the differential appears inside the limit.  Once we have a guess for the linear transformation $A \in L(\R^n, \R^m)$, though, we can use the definition above to check whether it's the differential.

For simple functions, we can make guesses based on our intuition from single-variable calculus.  For example, it shouldn't be surprising that a linear function is its own derivative; that is, the linear approximation to linear function is the function itself!

**Example:** Every linear transformation $T \in L(\R^n, \R^m)$ is its own differential, since

$$
\begin{aligned}
\lim_{h \rightarrow 0} \frac{\norm{T(x+h)-(Tx +Th)}}{\norm{h}}
&= \lim_{h \rightarrow 0} \frac{\norm{T(x+h)-T(x+h)}}{\norm{h}} = 0
\end{aligned}
$$

## Matrix-Valued Functions of a Scalar Variable

Let $A = [a_{ij}]$ be an $m \times n$ matrix whose elements are functions of the scalar parameter $\alpha$.  Then the derivative of $A$ with respect to $\alpha$ is the $m \times n$ matrix of element-by-element derivatives,

$$
d_\alpha A = 
\begin{bmatrix}
\diffd{a_{11}}{\alpha} & \diffd{a_{12}}{\alpha} & \cdots & \diffd{a_{1n}}{\alpha} \\
\diffd{a_{21}}{\alpha} & \diffd{a_{22}}{\alpha} & \cdots & \diffd{a_{2n}}{\alpha} \\
\vdots & \vdots && \vdots \\
\diffd{a_{m1}}{\alpha} & \diffd{a_{m2}}{\alpha} & \cdots & \diffd{a_{mn}}{\alpha} \\
\end{bmatrix}
$$

## Chain Rule for Total Derivatives

The most powerful tool for computing derivatives is the multivariable **chain rule**, which allows us to compute complicated derivatives from simpler building blocks.

**Theorem,** (Chain Rule).  Let $f : \R^n \rightarrow \R^k$ be differentiable at $x_0 \in \R^n$ and $g : \R^k \rightarrow \R^m$ be differentiable at $f(x_0) \in \R^k$.  Then the composite $h = g \circ f : \R^n \rightarrow \R^m$ given by $h(x) = g(f(x))$ is differentiable at $x_0$, with derivative

$$
h'(x_0) = g'(f(x_0)) \circ f'(x_0)
$$

that is, $h'(x_0)$ is the linear map obtained by composing the two linear maps $g'(f(x_0))$ and $f'(x_0)$ (or by multiplying the corresponding matrices!).

# Additional Reading

* [@barnes2006:matrix-differentiation] contains a list of helpful matrix calculus results
* [@golub2013:matrix-computations] contains many helpful linear algebra tricks